# Parameters:
# server_image: Docker image for the server (you can use any other image like apache, my_server_image, etc.).
# server_module: The Erlang module to run.
# server_function: The function inside the Erlang module to run.
# param1, param2, etc.: Parameters for the function.
# --rep: Number of repetitions.
# --file: Name of the CSV file to store results.
# --ports: Port mapping for the Docker container (default 8000:80).

# How to Run:
# python3 your_script.py --scaphandre_image hubblo/scaphandre --server_image nginx --server_name "nginx-custom" --num_requests 50000 --output_csv server_results.csv





import os
import time
import subprocess
import requests
import csv
from concurrent.futures import ThreadPoolExecutor
from collections import Counter
import argparse
import json

# Shared counters to track requests
results_counter = Counter()

def send_request(url):
    try:
        response = requests.get(url)
        if 200 <= response.status_code < 300:
            results_counter['success'] += 1
        else:
            results_counter['failure'] += 1
            print(f"Request failed: {response.status_code}, Response: {response.text}")
        results_counter['total'] += 1
    except requests.exceptions.RequestException as e:
        results_counter['failure'] += 1
        results_counter['total'] += 1
        print(f"Request exception: {e}")

def start_docker_containers(scaphandre_image, server_image, scaphandre_params, server_params):
    # Start Scaphandre container
    scaphandre_command = [
        "sudo", "docker", "run", "--privileged",
        "-v", "/sys/class/powercap:/sys/class/powercap",
        "-v", "/proc:/proc", "-ti", scaphandre_image
    ] + scaphandre_params

    scaphandre_process = subprocess.Popen(scaphandre_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    time.sleep(5)  # Allow some time for Scaphandre to initialize

    # Start the server container (could be any server, not just Nginx)
    server_command = ["sudo", "docker", "run", "-d", "-p", "8000:80", server_image] + server_params
    server_process = subprocess.Popen(server_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    time.sleep(5)  # Allow some time for the server to start

    return scaphandre_process, server_process

def stop_docker_containers(scaphandre_process, server_process):
    # Stop the containers after the test
    scaphandre_process.terminate()
    server_process.terminate()

def parse_json_and_compute_energy(file_name, server_name):
    # Read the JSON file generated by Scaphandre
    json_file_path = os.path.join(os.getcwd(), file_name)
    with open(json_file_path, "r") as file:
        data = json.load(file)

    # Initialize energy consumption variables
    total_server_consumption = 0.0
    number_samples = 0

    # Iterate through the JSON data and compute energy consumption for the server
    for entry in data:
        consumers = entry.get("consumers", [])
        for consumer in consumers:
            exe = consumer.get("exe", "")
            consumption = consumer.get("consumption", 0.0)

            # Check for consumption from the server process
            if server_name in exe.lower():  # Match the provided server name
                total_server_consumption += consumption
                number_samples += 1

    # Calculate average energy consumption if there are samples
    average_energy = total_server_consumption / number_samples if number_samples != 0 else 0

    return total_server_consumption, average_energy, number_samples

def save_results_to_csv(filename, results, total_energy, average_energy, total_runtime, total_samples):
    # Save results to CSV
    headers = ["Total Requests", "Successful Requests", "Failed Requests", "Execution Time (seconds)",
               "Total Energy Consumption (J)", "Average Energy Consumption (J)", "Number of Samples"]
    data = [
        [
            results['total'],
            results['success'],
            results['failure'],
            total_runtime,
            total_energy,
            average_energy,
            total_samples
        ]
    ]
    
    # Writing to CSV file
    with open(filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        writer.writerow(headers)
        writer.writerows(data)
    print(f"Results saved to {filename}")

def main():
    # Argument parsing for configuration
    parser = argparse.ArgumentParser(description="Measure energy consumption of a web server with Scaphandre.")
    parser.add_argument('--scaphandre_image', type=str, default='hubblo/scaphandre', help="Scaphandre Docker image")
    parser.add_argument('--server_image', type=str, default='nginx', help="Server Docker image")
    parser.add_argument('--server_name', type=str, default=None, help="Name of the server process to track for energy consumption")
    parser.add_argument('--scaphandre_params', type=str, nargs='*', default=['json', '-f', 'testyaw.json'],
                        help="Additional parameters for the Scaphandre container")
    parser.add_argument('--server_params', type=str, nargs='*', default=[],
                        help="Additional parameters for the server container")
    parser.add_argument('--num_requests', type=int, default=50000, help="Number of requests to send")
    parser.add_argument('--output_csv', type=str, default='server_results.csv', help="Output CSV file name")
    
    args = parser.parse_args()

    url = "http://localhost:8000/"
    n = args.num_requests  # Number of requests to send

    # If server_name is not provided, default to the server_image name
    server_name = args.server_name if args.server_name else args.server_image

    # Start Docker containers
    print("Starting Docker containers...")
    scaphandre_process, server_process = start_docker_containers(
        args.scaphandre_image, args.server_image, args.scaphandre_params, args.server_params
    )

    # Wait a bit for the server and Scaphandre to be ready
    print(f"Sending {n} requests to {url}...")
    time.sleep(10)  # Allow time for the server to start

    # Measure the runtime
    start_time = time.time()

    # Send requests using ThreadPoolExecutor for concurrency
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(send_request, url) for _ in range(n)]
        for _ in futures:
            _.result()  # Wait for all requests to complete

    # Calculate the runtime
    end_time = time.time()
    runtime = end_time - start_time
    results_counter['runtime'] = runtime

    # Parse the JSON output and compute energy consumption statistics
    total_energy, average_energy, total_samples = parse_json_and_compute_energy('testyaw.json', server_name)

    # Save all results to CSV
    save_results_to_csv(args.output_csv, results_counter, total_energy, average_energy, runtime, total_samples)

    # Summary of the results
    print("\nSummary:")
    print(f"Total Requests Sent: {results_counter['total']}")
    print(f"Successful Requests: {results_counter['success']}")
    print(f"Failed Requests: {results_counter['failure']}")
    print(f"Execution Time: {runtime:.2f} seconds")
    print(f"Total Energy Consumption: {total_energy:.2f} J")
    print(f"Average Energy Consumption: {average_energy:.2f} J")

    # Stop Docker containers
    print("Stopping Docker containers...")
    stop_docker_containers(scaphandre_process, server_process)

if __name__ == "__main__":
    main()
